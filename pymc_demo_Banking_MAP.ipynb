{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import pickle\n",
    "\n",
    "pd.options.display.max_columns=150\n",
    "pd.options.display.max_rows=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('banking_sector_weekly.csv')\n",
    "df.dropna(subset=['vol', 'eps','weekly_return', 'prior_wk_ret1', 'prior_wk_ret2',\n",
    "       'prior_wk_ret3', 'prior_wk_ret4', 'prior_wk_ret5', 'prior_wk_ret6',\n",
    "       'prior_wk_ret7', 'imports', 'exports', 'payroll', 'unemployment',\n",
    "       'unemployment_delta'],inplace=True)\n",
    "df['beg_date'] = pd.to_datetime(df['beg_date'])\n",
    "\n",
    "df['ML3_return'] = (df['prior_wk_ret1']+df['prior_wk_ret2']+df['prior_wk_ret3'])/3.0\n",
    "df['ML5_return'] = (df['prior_wk_ret1']+df['prior_wk_ret2']+df['prior_wk_ret3']+df['prior_wk_ret4']+df['prior_wk_ret5'])/5.0\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[['vol', 'eps', 'prior_wk_ret1', 'prior_wk_ret2',\n",
    "       'prior_wk_ret3', 'prior_wk_ret4', 'prior_wk_ret5', 'prior_wk_ret6',\n",
    "       'prior_wk_ret7', 'imports', 'exports', 'payroll', 'unemployment',\n",
    "       'unemployment_delta']] = scaler.fit_transform(df[['vol', 'eps', 'prior_wk_ret1', 'prior_wk_ret2',\n",
    "       'prior_wk_ret3', 'prior_wk_ret4', 'prior_wk_ret5', 'prior_wk_ret6',\n",
    "       'prior_wk_ret7', 'imports', 'exports', 'payroll', 'unemployment',\n",
    "       'unemployment_delta']])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first try fully pooled to see what's up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[(df['beg_date']>pd.to_datetime('2014-01-01'))&(df['beg_date']<=pd.to_datetime('2017-01-01'))]\n",
    "test_df = df[(df['beg_date']>=pd.to_datetime('2017-01-01'))]\n",
    "\n",
    "x = train_df[['eps', 'prior_wk_ret1', 'prior_wk_ret2',\n",
    "       'prior_wk_ret3', 'prior_wk_ret4', 'prior_wk_ret5', 'prior_wk_ret6',\n",
    "       'prior_wk_ret7', 'imports', 'exports', 'payroll', 'unemployment',\n",
    "       'unemployment_delta','ML3_return','ML5_return']]\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "y = train_df[['weekly_return']]\n",
    "\n",
    "lm = sm.OLS(y,x)\n",
    "results = lm.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['pred'] = results.predict(x)\n",
    "\n",
    "train_df.to_csv('ols_trainset_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_df[['eps', 'prior_wk_ret1', 'prior_wk_ret2',\n",
    "       'prior_wk_ret3', 'prior_wk_ret4', 'prior_wk_ret5', 'prior_wk_ret6',\n",
    "       'prior_wk_ret7', 'imports', 'exports', 'payroll', 'unemployment',\n",
    "       'unemployment_delta','ML3_return','ML5_return']]\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "test_df['pred'] = results.predict(x)\n",
    "\n",
    "test_df.to_csv('ols_testset_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalisation results are NAT good. Let us try partial pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "df['conm_code'] = le.fit_transform(df.tic)\n",
    "df['weekly_return'] = df['weekly_return'].astype(theano.config.floatX)\n",
    "\n",
    "\n",
    "train_df = df[(df['beg_date']>pd.to_datetime('2014-01-01'))&(df['beg_date']<=pd.to_datetime('2017-01-01'))]\n",
    "test_df = df[(df['beg_date']>=pd.to_datetime('2017-01-01'))]\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "relevant_vars=['eps', 'prior_wk_ret1', 'prior_wk_ret2',\n",
    "       'prior_wk_ret3', 'prior_wk_ret4', 'prior_wk_ret5', 'prior_wk_ret6',\n",
    "       'prior_wk_ret7', 'imports', 'exports', 'payroll', 'unemployment',\n",
    "       'unemployment_delta','ML3_return','ML5_return']\n",
    "\n",
    "conm_idx = train_df.conm_code.values\n",
    "n_companies = len(df.conm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conm_idx = theano.shared(np.array(train_df.conm_code.values,dtype='int32'))\n",
    "\n",
    "x1_shared = theano.shared(np.array(train_df['prior_wk_ret1'],dtype='float32'))\n",
    "x2_shared = theano.shared(np.array(train_df['prior_wk_ret2'],dtype='float32'))\n",
    "x3_shared = theano.shared(np.array(train_df['prior_wk_ret3'],dtype='float32'))\n",
    "x4_shared = theano.shared(np.array(train_df['prior_wk_ret4'],dtype='float32'))\n",
    "x5_shared = theano.shared(np.array(train_df['prior_wk_ret5'],dtype='float32'))\n",
    "x6_shared = theano.shared(np.array(train_df['prior_wk_ret6'],dtype='float32'))\n",
    "x7_shared = theano.shared(np.array(train_df['prior_wk_ret7'],dtype='float32'))\n",
    "x8_shared = theano.shared(np.array(train_df['eps'],dtype='float32'))\n",
    "x9_shared = theano.shared(np.array(train_df['imports'],dtype='float32'))\n",
    "x10_shared = theano.shared(np.array(train_df['exports'],dtype='float32'))\n",
    "x11_shared = theano.shared(np.array(train_df['unemployment'],dtype='float32'))\n",
    "x12_shared = theano.shared(np.array(train_df['unemployment_delta'],dtype='float32'))\n",
    "x13_shared = theano.shared(np.array(train_df['ML3_return'],dtype='float32'))\n",
    "x14_shared = theano.shared(np.array(train_df['ML5_return'],dtype='float32'))\n",
    "#ann_output = theano.shared(train_df['weekly_return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model:\n",
    "    # Hyperpriors for group nodes\n",
    "    mu_a = pm.Normal('mu_a', mu=0., sd=100000**2)\n",
    "    sigma_a = pm.HalfCauchy('sigma_a', 50)\n",
    "    mu_b = pm.Normal('mu_b', mu=0., sd=100000**2)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', 50)\n",
    "    mu_b1 = pm.Normal('mu_b1', mu=0., sd=100000**2)\n",
    "    sigma_b1 = pm.HalfCauchy('sigma_b1', 50)\n",
    "    mu_b2 = pm.Normal('mu_b2', mu=0., sd=100000**2)\n",
    "    sigma_b2 = pm.HalfCauchy('sigma_b2', 50)\n",
    "    mu_b3 = pm.Normal('mu_b3', mu=0., sd=100000**2)\n",
    "    sigma_b3 = pm.HalfCauchy('sigma_b3', 50)\n",
    "    mu_b4 = pm.Normal('mu_b4', mu=0., sd=100000**2)\n",
    "    sigma_b4 = pm.HalfCauchy('sigma_b4', 50)\n",
    "    mu_b5 = pm.Normal('mu_b5', mu=0., sd=100000**2)\n",
    "    sigma_b5 = pm.HalfCauchy('sigma_b5', 50)\n",
    "    mu_b6 = pm.Normal('mu_b6', mu=0., sd=100000**2)\n",
    "    sigma_b6 = pm.HalfCauchy('sigma_b6', 50)\n",
    "    mu_b7 = pm.Normal('mu_b7', mu=0., sd=100000**2)\n",
    "    sigma_b7 = pm.HalfCauchy('sigma_b7', 50)\n",
    "    mu_b8 = pm.Normal('mu_b8', mu=0., sd=100000**2)\n",
    "    sigma_b8 = pm.HalfCauchy('sigma_b8', 50)\n",
    "    mu_b9 = pm.Normal('mu_b9', mu=0., sd=100000**2)\n",
    "    sigma_b9 = pm.HalfCauchy('sigma_b9', 50)\n",
    "    mu_b10 = pm.Normal('mu_b10', mu=0., sd=100000**2)\n",
    "    sigma_b10 = pm.HalfCauchy('sigma_b10', 50)\n",
    "    mu_b11 = pm.Normal('mu_b11', mu=0., sd=100000**2)\n",
    "    sigma_b11 = pm.HalfCauchy('sigma_b11', 50)\n",
    "    mu_b12 = pm.Normal('mu_b12', mu=0., sd=100000**2)\n",
    "    sigma_b12 = pm.HalfCauchy('sigma_b12', 50)\n",
    "    mu_b13 = pm.Normal('mu_b13', mu=0., sd=100000**2)\n",
    "    sigma_b13 = pm.HalfCauchy('sigma_b13', 50)\n",
    "    #mu_b14 = pm.Normal('mu_b14', mu=0., sd=100**2)\n",
    "    #sigma_b14 = pm.HalfCauchy('sigma_b14', 5)\n",
    "    \n",
    "\n",
    "    # Intercept for each conm, distributed around group mean mu_a\n",
    "    # Above we just set mu and sd to a fixed value while here we\n",
    "    # plug in a common group distribution for all a and b (which are\n",
    "    # vectors of length n_companies).\n",
    "    a = pm.Normal('a', mu=mu_a, sd=sigma_a, shape=n_companies)\n",
    "    # beta 1 through 6\n",
    "    b = pm.Normal('b', mu=mu_b, sd=sigma_b, shape=n_companies)\n",
    "    b1 = pm.Normal('b1', mu=mu_b1, sd=sigma_b1, shape=n_companies)\n",
    "    b2 = pm.Normal('b2', mu=mu_b2, sd=sigma_b2, shape=n_companies)\n",
    "    b3 = pm.Normal('b3', mu=mu_b3, sd=sigma_b3, shape=n_companies)\n",
    "    b4 = pm.Normal('b4', mu=mu_b4, sd=sigma_b4, shape=n_companies)\n",
    "    b5 = pm.Normal('b5', mu=mu_b5, sd=sigma_b5, shape=n_companies)\n",
    "    b6 = pm.Normal('b6', mu=mu_b6, sd=sigma_b6, shape=n_companies)\n",
    "    b7 = pm.Normal('b7', mu=mu_b7, sd=sigma_b7, shape=n_companies)\n",
    "    b8 = pm.Normal('b8', mu=mu_b8, sd=sigma_b8, shape=n_companies)\n",
    "    b9 = pm.Normal('b9', mu=mu_b9, sd=sigma_b9, shape=n_companies)\n",
    "    b10 = pm.Normal('b10', mu=mu_b10, sd=sigma_b10, shape=n_companies)\n",
    "    b11 = pm.Normal('b11', mu=mu_b11, sd=sigma_b11, shape=n_companies)\n",
    "    b12 = pm.Normal('b12', mu=mu_b12, sd=sigma_b12, shape=n_companies)\n",
    "    b13 = pm.Normal('b13', mu=mu_b13, sd=sigma_b13, shape=n_companies)\n",
    "    #b14 = pm.Normal('b14', mu=mu_b14, sd=sigma_b14, shape=n_companies)\n",
    "\n",
    "    # Model error\n",
    "    #['eps', 'prior_wk_ret1', 'prior_wk_ret2',\n",
    "    #   'prior_wk_ret3', 'prior_wk_ret4', 'prior_wk_ret5', 'prior_wk_ret6',\n",
    "    #   'prior_wk_ret7', 'imports', 'exports', 'payroll', 'unemployment',\n",
    "    #   'unemployment_delta','ML3_return','ML5_return']\n",
    "    \n",
    "    sigma = pm.Gamma(\"sigma\", alpha=10, beta=1)\n",
    "    \n",
    "    #dir_est=a[conm_idx] + b[conm_idx] * x1_shared + b1[conm_idx] * x2_shared + \\\n",
    "    #         b2[conm_idx] * x3_shared + b3[conm_idx] * x4_shared + \\\n",
    "    #         b4[conm_idx] * x5_shared + b5[conm_idx] * x6_shared + \\\n",
    "    #        b6[conm_idx] * x7_shared + b7[conm_idx] * x8_shared + \\\n",
    "    #        b8[conm_idx] * x9_shared + b9[conm_idx] * x10_shared + \\\n",
    "    #        b10[conm_idx] * x11_shared + b11[conm_idx] * x12_shared + \\\n",
    "    #        b12[conm_idx] * x13_shared + b13[conm_idx] * x14_shared\n",
    "    \n",
    "    dir_est=a[conm_idx] + b[conm_idx] * x1_shared + b7[conm_idx] * x8_shared + \\\n",
    "            b8[conm_idx] * x9_shared + b9[conm_idx] * x10_shared + \\\n",
    "            b10[conm_idx] * x11_shared + b11[conm_idx] * x12_shared + \\\n",
    "            b12[conm_idx] * x13_shared + b13[conm_idx] * x14_shared\n",
    "\n",
    "\n",
    "    # Data likelihood\n",
    "    dir_like = pm.Normal('dir_like', mu=dir_est, sd=sigma, observed=train_df.weekly_return)\n",
    "    #dir_like = pm.Logistic('dir_like', mu=dir_est, s=sigma, observed=data_train.weekly_direction)\n",
    "    #dir_like = pm.Bernoulli('dir_like',p=dir_est, observed=data_train.weekly_direction)\n",
    "    \n",
    "    start = pm.find_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start['b13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conm_idx = test_df.conm_code.values\n",
    "\n",
    "preds = start['a'][conm_idx] + start['b'][conm_idx] * test_df['prior_wk_ret1'] + start['b7'][conm_idx] * test_df['eps'] + \\\n",
    "            start['b8'][conm_idx] * test_df['imports'] + start['b9'][conm_idx] * test_df['exports'] + \\\n",
    "            start['b10'][conm_idx] * test_df['unemployment'] + start['b11'][conm_idx] * test_df['unemployment_delta'] + \\\n",
    "            start['b12'][conm_idx] * test_df['ML3_return'] + start['b13'][conm_idx] * test_df['ML5_return']\n",
    "\n",
    "test_df['preds'] = preds\n",
    "\n",
    "test_df.to_csv('bhm_testset_results_MAP.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
